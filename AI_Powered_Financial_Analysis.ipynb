{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install newspaper3k\n",
        "!pip install lxml[html_clean]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "7Tpko84YpDiX",
        "outputId": "11d54d1b-e281-4671-fe6c-104ec29d09fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'!pip install newspaper3k\\n!pip install lxml[html_clean]'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "cnbc_url = \"https://www.cnbc.com/finance/\"\n",
        "headers_dict = {\"User-Agent\": \"Mozilla/5.0\"}"
      ],
      "metadata": {
        "id": "g_KgRxmspDfx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_cnbc_article_links(target_url=cnbc_url):\n",
        "\n",
        "    page_response = requests.get(target_url, headers=headers_dict)\n",
        "\n",
        "    html_soup = BeautifulSoup(page_response.content, \"html.parser\")\n",
        "\n",
        "\n",
        "    unique_links = set()\n",
        "\n",
        "    for link_tag in html_soup.find_all(\"a\", href=True):\n",
        "        url = link_tag[\"href\"]\n",
        "\n",
        "\n",
        "        if url.startswith(\"https://www.cnbc.com/\") and \"/202\" in url:\n",
        "\n",
        "           unique_links.add(url)\n",
        "\n",
        "    return list(unique_links)"
      ],
      "metadata": {
        "id": "i6gc5CEypDcJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnbc_links = fetch_cnbc_article_links()\n",
        "\n",
        "print(f\"Total articles fetched: {len(cnbc_links)} \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UH1JT7wnpDYC",
        "outputId": "26371733-614e-492a-9c6e-4b9dcaa38b40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total articles fetched: 38 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "session = SparkSession.builder \\\n",
        "    .appName(\"FinInsight_Pipeline\") \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .config(\"spark.ui.port\", \"4050\") \\\n",
        "    .config(\"spark.driver.memory\", \"4G\") \\\n",
        "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
        "    .config(\"spark.kryoserializer.buffer.max\", \"1000M\") \\\n",
        "    .config(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp_2.12:5.1.3\") \\\n",
        "    .getOrCreate()\n",
        "\n"
      ],
      "metadata": {
        "id": "K6mE01LypDUw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "article_links_list = cnbc_links"
      ],
      "metadata": {
        "id": "Z5QWY4bDpDP4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parallelized_urls = session.sparkContext.parallelize(article_links_list)"
      ],
      "metadata": {
        "id": "kNF2XBDJpDLz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install newspaper3k\n",
        "!pip install lxml[html_clean]\n",
        "from newspaper import Article"
      ],
      "metadata": {
        "id": "1twccvStpC8U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61eb2e19-fba3-4152-d609-7987efb13a85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: newspaper3k in /usr/local/lib/python3.11/dist-packages (0.2.8)\n",
            "Requirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (4.13.4)\n",
            "Requirement already satisfied: Pillow>=3.3.0 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (11.1.0)\n",
            "Requirement already satisfied: PyYAML>=3.11 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (6.0.2)\n",
            "Requirement already satisfied: cssselect>=0.9.2 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (1.3.0)\n",
            "Requirement already satisfied: lxml>=3.6.0 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (5.3.2)\n",
            "Requirement already satisfied: nltk>=3.2.1 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (3.9.1)\n",
            "Requirement already satisfied: requests>=2.10.0 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (2.32.3)\n",
            "Requirement already satisfied: feedparser>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (6.0.11)\n",
            "Requirement already satisfied: tldextract>=2.0.1 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (5.2.0)\n",
            "Requirement already satisfied: feedfinder2>=0.0.4 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (0.0.4)\n",
            "Requirement already satisfied: jieba3k>=0.35.1 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (0.35.1)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (2.8.2)\n",
            "Requirement already satisfied: tinysegmenter==0.3 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (0.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.4.1->newspaper3k) (2.6)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.4.1->newspaper3k) (4.13.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from feedfinder2>=0.0.4->newspaper3k) (1.17.0)\n",
            "Requirement already satisfied: sgmllib3k in /usr/local/lib/python3.11/dist-packages (from feedparser>=5.2.1->newspaper3k) (1.0.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>=3.2.1->newspaper3k) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>=3.2.1->newspaper3k) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>=3.2.1->newspaper3k) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk>=3.2.1->newspaper3k) (4.67.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.10.0->newspaper3k) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.10.0->newspaper3k) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.10.0->newspaper3k) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.10.0->newspaper3k) (2025.1.31)\n",
            "Requirement already satisfied: requests-file>=1.4 in /usr/local/lib/python3.11/dist-packages (from tldextract>=2.0.1->newspaper3k) (2.1.0)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.11/dist-packages (from tldextract>=2.0.1->newspaper3k) (3.18.0)\n",
            "Requirement already satisfied: lxml[html_clean] in /usr/local/lib/python3.11/dist-packages (5.3.2)\n",
            "Requirement already satisfied: lxml_html_clean in /usr/local/lib/python3.11/dist-packages (from lxml[html_clean]) (0.4.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_article_details(link):\n",
        "    try:\n",
        "        article_obj = Article(link)\n",
        "        article_obj.download()\n",
        "        article_obj.parse()\n",
        "\n",
        "        return {\n",
        "            \"url\": link,\n",
        "            \"title\":  article_obj.title,\n",
        "            \"date\": str( article_obj.publish_date),\n",
        "            \"content\":  article_obj.text\n",
        "        }\n",
        "\n",
        "    except Exception:\n",
        "        return None"
      ],
      "metadata": {
        "id": "2hyQcvd7pCv2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_articles_rdd = parallelized_urls.map(get_article_details).filter(lambda record: record is not None)"
      ],
      "metadata": {
        "id": "kaq_CCsepCin"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import StructType, StructField, StringType\n",
        "\n",
        "article_structure= StructType([\n",
        "\n",
        "    StructField(\"url\", StringType(), True),\n",
        "\n",
        "    StructField(\"title\", StringType(), True),\n",
        "        StructField(\"date\", StringType(), True),\n",
        "    StructField(\"content\", StringType(), True),\n",
        "])"
      ],
      "metadata": {
        "id": "om8BDfnOqL3b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "article_dataframe= session.createDataFrame(filtered_articles_rdd, schema=article_structure)"
      ],
      "metadata": {
        "id": "FB-K0wOlpkug"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "article_dataframe.show(truncate=100)\n",
        "\n",
        "article_dataframe.write.mode(\"overwrite\").json(\"output/realtime_financial_data\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lPcmv2m1pliU",
        "outputId": "46ce417b-a66c-4e55-c021-089f81eb92ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------+-------------------+----------------------------------------------------------------------------------------------------+\n",
            "|                                                                                                 url|                                                                                               title|               date|                                                                                             content|\n",
            "+----------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------+-------------------+----------------------------------------------------------------------------------------------------+\n",
            "|https://www.cnbc.com/2025/04/16/4-money-traps-to-avoid-in-the-market-fast-money-trader-tim-seymou...|        Four money traps to avoid in a volatile market, according to ‘Fast Money’ trader Tim Seymour|2025-04-16 00:00:00|\"Fast Money\" trader Tim Seymour wants to help investors avoid common money traps that could leave...|\n",
            "|                   https://www.cnbc.com/2025/04/17/us-stocks-international-stocks-trump-tariffs.html|        Should investors dump U.S. stocks for international equities? Here's what experts are saying|2025-04-17 00:00:00|watch now\\n\\nSome investors accustomed to the dominance of U.S. stocks versus the rest of the wor...|\n",
            "|           https://www.cnbc.com/2025/04/15/stocks-making-the-biggest-moves-premarket-bac-ba-jnj.html|      Stocks making the biggest moves premarket: Bank of America, Boeing, Johnson & Johnson and more|2025-04-15 00:00:00|Check out the companies making headlines before the bell: Bank of America — Shares rose about 2% ...|\n",
            "|https://www.cnbc.com/2025/04/16/-hertz-surges-after-bill-ackman-takes-big-stake-in-the-rental-car...|               Hertz shares surge more than 50% after Bill Ackman takes big stake in rental car firm|2025-04-16 00:00:00|Shares of Hertz surged 56% on Wednesday after a regulatory filing revealed Pershing Square had bu...|\n",
            "|           https://www.cnbc.com/2025/04/16/stocks-making-the-biggest-moves-midday-nvda-asml-amd.html|               Stocks making the biggest moves midday: Nvidia, ASML, Advanced Micro Devices and more|2025-04-16 00:00:00|Check out the companies making headlines in midday trading: Nvidia — Shares tumbled 7% after Nvid...|\n",
            "|https://www.cnbc.com/2025/04/16/stocks-making-the-biggest-moves-premarket-ual-nvda-tsla-trv-and-m...|       Stocks making the biggest moves premarket: United Airlines, Nvidia, Tesla, Travelers and more|2025-04-16 00:00:00|Check out the companies making headlines in premarket trading: Nvidia — Shares slipped more than ...|\n",
            "|https://www.cnbc.com/2025/04/18/capital-one-and-discover-merger-approved-by-federal-reserve-board...|                                         Capital One and Discover merger approved by Federal Reserve|2025-04-18 00:00:00|Capital One Financial's application to acquire Discover Financial Services in a $35.3 billion all...|\n",
            "|https://www.cnbc.com/2025/04/17/american-express-cardholders-still-spending-despite-trump-tariffs...|                        American Express’ wealthy cardholders are mostly untouched by tariff jitters|2025-04-17 00:00:00|American Express ' affluent cardholders are showing few signs of curbing their spending, and youn...|\n",
            "|          https://www.cnbc.com/2025/04/17/stocks-making-the-biggest-moves-premarket-htz-unh-lly.html|                  Stocks making the biggest moves premarket: Hertz, UnitedHealth, Eli Lilly and more|2025-04-17 00:00:00|Check out the companies making headlines before the bell: Hertz — Shares of the rental car compan...|\n",
            "|https://www.cnbc.com/2025/04/19/trumps-approval-rating-on-economy-drops-to-lowest-of-his-presiden...|Trump's approval rating on the economy drops to lowest of his presidential career, CNBC Survey finds|2025-04-19 00:00:00|U.S. President Donald Trump speaks as he meets with Italian Prime Minister Giorgia Meloni (not pi...|\n",
            "|          https://www.cnbc.com/2025/04/15/cash-may-feel-safe-when-stocks-slide-but-it-has-risks.html|                                              Cash may feel safe when stocks slide, but it has risks|2025-04-15 00:00:00|Traders work on the floor of the New York Stock Exchange on April 10, 2025. Michael M. Santiago |...|\n",
            "|     https://www.cnbc.com/2025/04/20/chinese-stocks-that-could-survive-delisting-tariff-worries.html|                    Bernstein predicts which Chinese stocks can survive delisting and tariff worries|2025-04-20 00:00:00|Bernstein is looking at Chinese internet tech stocks like it's the downtrodden days of Covid-19. ...|\n",
            "|https://www.cnbc.com/2025/04/21/china-to-retaliate-against-nations-that-work-with-us-to-isolate-b...|                  China vows retaliation against countries that follow U.S. calls to isolate Beijing|2025-04-21 00:00:00|BEIJING — China on Monday warned it will retaliate against countries that cooperate with the U.S....|\n",
            "|https://www.cnbc.com/2025/04/17/ubs-loses-crown-as-continental-europes-most-valuable-bank-to-sant...|           UBS loses crown as continental Europe's most valuable bank to Santander amid U.S. tariffs|2025-04-17 00:00:00|Spanish lender Banco Santander has eclipsed Swiss giant UBS as continental Europe's largest bank ...|\n",
            "|                 https://www.cnbc.com/2025/04/16/heres-why-retirees-shouldnt-fully-ditch-stocks.html|                                                    Here's why retirees shouldn't fully ditch stocks|2025-04-16 00:00:00|Lordhenrivoton | E+ | Getty Images\\n\\nRetirees may think moving all their investments to cash and...|\n",
            "|https://www.cnbc.com/2025/04/14/nyse-president-systems-are-handling-record-volume-more-efficientl...|Op-ed: NYSE systems are working normally, handling record volume more efficiently than during Cov...|2025-04-14 00:00:00|The great American entrepreneur Henry Ford once said, \"The only real security that a man can have...|\n",
            "|             https://www.cnbc.com/2025/04/16/jpmorgan-chase-infinite-money-glitch-bank-lawsuits.html|              JPMorgan Chase sues more customers who allegedly stole cash in 'infinite money glitch'|2025-04-16 00:00:00|A person uses an ATM at a Chase bank in New York City on November 19, 2024.\\n\\nJPMorgan Chase thi...|\n",
            "|                       https://www.cnbc.com/2025/04/18/where-made-in-china-2025-missed-the-mark.html|                                                          Where 'Made in China 2025' missed the mark|2025-04-18 00:00:00|Smart robotic arms work on the production line at the production workshop of Changqing Auto Parts...|\n",
            "|https://www.cnbc.com/2025/04/17/trump-again-calls-for-fed-to-cut-rates-says-powells-termination-c...|         Trump again calls for Fed to cut rates, says Powell’s ‘termination cannot come fast enough’|2025-04-17 00:00:00|President Donald Trump on Thursday again called for the Federal Reserve to lower rates and even h...|\n",
            "|https://www.cnbc.com/2025/04/19/investor-protection-during-market-volatility-through-tactical-fun...|                                This fund is designed to help investors withstand wild market swings|2025-04-19 00:00:00|Katie Stockton thinks she has a viable option for investors trying to withstand wild market swing...|\n",
            "+----------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------+-------------------+----------------------------------------------------------------------------------------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install praw\n",
        "!pip install asyncpraw"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xg-NSt7Nplfk",
        "outputId": "687e089a-e956-45c0-a34b-927b78a5ccbc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: praw in /usr/local/lib/python3.11/dist-packages (7.8.1)\n",
            "Requirement already satisfied: prawcore<3,>=2.4 in /usr/local/lib/python3.11/dist-packages (from praw) (2.4.0)\n",
            "Requirement already satisfied: update_checker>=0.18 in /usr/local/lib/python3.11/dist-packages (from praw) (0.18.0)\n",
            "Requirement already satisfied: websocket-client>=0.54.0 in /usr/local/lib/python3.11/dist-packages (from praw) (1.8.0)\n",
            "Requirement already satisfied: requests<3.0,>=2.6.0 in /usr/local/lib/python3.11/dist-packages (from prawcore<3,>=2.4->praw) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2025.1.31)\n",
            "Requirement already satisfied: asyncpraw in /usr/local/lib/python3.11/dist-packages (7.8.1)\n",
            "Requirement already satisfied: aiofiles in /usr/local/lib/python3.11/dist-packages (from asyncpraw) (24.1.0)\n",
            "Requirement already satisfied: aiohttp<4 in /usr/local/lib/python3.11/dist-packages (from asyncpraw) (3.11.15)\n",
            "Requirement already satisfied: aiosqlite<=0.17.0 in /usr/local/lib/python3.11/dist-packages (from asyncpraw) (0.17.0)\n",
            "Requirement already satisfied: asyncprawcore<3,>=2.4 in /usr/local/lib/python3.11/dist-packages (from asyncpraw) (2.4.0)\n",
            "Requirement already satisfied: update_checker>=0.18 in /usr/local/lib/python3.11/dist-packages (from asyncpraw) (0.18.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4->asyncpraw) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4->asyncpraw) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4->asyncpraw) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4->asyncpraw) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4->asyncpraw) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4->asyncpraw) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4->asyncpraw) (1.19.0)\n",
            "Requirement already satisfied: typing_extensions>=3.7.2 in /usr/local/lib/python3.11/dist-packages (from aiosqlite<=0.17.0->asyncpraw) (4.13.2)\n",
            "Requirement already satisfied: requests>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from update_checker>=0.18->asyncpraw) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.3.0->update_checker>=0.18->asyncpraw) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.3.0->update_checker>=0.18->asyncpraw) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.3.0->update_checker>=0.18->asyncpraw) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.3.0->update_checker>=0.18->asyncpraw) (2025.1.31)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import praw\n",
        "import warnings\n",
        "from datetime import datetime\n",
        "\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"praw\")\n",
        "\n",
        "def fetch_subreddit_posts(subreddit=\"wallstreetbets\", max_posts=500):\n",
        "    \"\"\"\n",
        "    Connect to Reddit and fetch posts from a given subreddit.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    reddit_api = praw.Reddit(\n",
        "        client_id='MKu1HwTXIGSooUuXaOrzIQ',\n",
        "        client_secret='sMSkoJf9JM_XON77U-ppAgpb4A-KiA',\n",
        "        user_agent='reddit_fetcher:v1.0 (by u/Informal-Muffin5689)'\n",
        "    )\n",
        "\n",
        "    fetched_posts = []\n",
        "\n",
        "    for item in list(reddit_api.subreddit(subreddit).new(limit=max_posts)):\n",
        "        fetched_posts.append({\n",
        "\n",
        "            \"Title\": item.title,\n",
        "            \"URL\": item.url,\n",
        "            \"Upvotes\": item.score,\n",
        "            \"Comments_Count\": item.num_comments,\n",
        "            \"Post_Time\": datetime.utcfromtimestamp(item.created_utc).strftime('%Y-%m-%d %H:%M:%S')\n",
        "        })\n",
        "    return fetched_posts"
      ],
      "metadata": {
        "id": "zhz6kZkVplaI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType\n",
        "\n",
        "reddit_session = SparkSession.builder.appName(\"RedditETLPipeline\").getOrCreate()\n",
        "\n",
        "data_dir = \"/content/reddit_data_raw\"\n",
        "if os.path.exists(data_dir):\n",
        "    shutil.rmtree(data_dir)\n",
        "\n",
        "schema_reddit_posts = StructType([\n",
        "    StructField(\"Title\", StringType(), True),\n",
        "    StructField(\"URL\", StringType(), True),\n",
        "    StructField(\"Upvotes\", IntegerType(), True),\n",
        "    StructField(\"Comments_Count\", IntegerType(), True),\n",
        "    StructField(\"Post_Time\", StringType(), True)\n",
        "])"
      ],
      "metadata": {
        "id": "dd9sEbM9plUf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "raw_data = fetch_subreddit_posts()\n",
        "\n",
        "\n",
        "if raw_data:\n",
        "\n",
        "\n",
        "    reddit_df = reddit_session.createDataFrame(\n",
        "        [(p[\"Title\"], p[\"URL\"], p[\"Upvotes\"], p[\"Comments_Count\"], p[\"Post_Time\"]) for p in raw_data],\n",
        "        schema=schema_reddit_posts\n",
        "    )\n",
        "\n",
        "    reddit_df = reddit_df.withColumn(\"Post_Time\", col(\"Post_Time\").cast(TimestampType()))\n",
        "\n",
        "\n",
        "    reddit_df.show(5, truncate=True)\n",
        "\n",
        "    reddit_df.coalesce(1).write \\\n",
        "        .mode(\"overwrite\") \\\n",
        "        .format(\"json\") \\\n",
        "        .option(\"compression\", \"none\") \\\n",
        "        .save(data_dir)\n",
        "\n",
        "\n",
        "\n",
        "    print(f\"✔️ Stored {reddit_df.count()} posts at {data_dir}\")\n",
        "else:\n",
        "    print(\"⚠️ No Reddit data retrieved.\")\n",
        "\n",
        "\n",
        "reddit_session.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OEzaGuAdpwPF",
        "outputId": "3995f772-9d19-404b-9f09-8443ab6b1f46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+-------+--------------+-------------------+\n",
            "|               Title|                 URL|Upvotes|Comments_Count|          Post_Time|\n",
            "+--------------------+--------------------+-------+--------------+-------------------+\n",
            "|        Calls on GLD|https://i.redd.it...|      1|             0|2025-04-21 07:29:17|\n",
            "|Selling my feet i...|https://i.redd.it...|      0|             6|2025-04-21 07:11:01|\n",
            "|Expected interest...|https://i.redd.it...|     20|            24|2025-04-21 06:21:39|\n",
            "|Buying TSLA calls...|https://www.reddi...|    228|            92|2025-04-21 06:03:10|\n",
            "|                Hmmm|https://www.reddi...|     17|            17|2025-04-21 05:19:12|\n",
            "+--------------------+--------------------+-------+--------------+-------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "✔️ Stored 500 posts at /content/reddit_data_raw\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q pyspark"
      ],
      "metadata": {
        "id": "aqLQJcIx37hI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q 'https://drive.google.com/uc?export=download&id=11JzGCYd4PNJgQDAG7YSxZ6zDohLnbf1U' -O 'SEC_filings.csv'"
      ],
      "metadata": {
        "id": "qPK5pl8emEw_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "if os.path.exists('SEC_filings.csv'):\n",
        "    print(\"SEC_filings.csv has been downloaded successfully.\")\n",
        "else:\n",
        "    print(\"SEC_filings.csv has not been downloaded\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RZN9gfe_mGmU",
        "outputId": "0ef80bff-bc8f-4e14-9025-e5f20727f835"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SEC_filings.csv has been downloaded successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext, SparkConf\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "conf = SparkConf().setAppName('SEC_Filings_Ingestion_Pipeline') \\\n",
        "    .set(\"spark.driver.memory\", \"4g\") \\\n",
        "    .set(\"spark.executor.memory\", \"4g\")\n",
        "sc = SparkContext.getOrCreate(conf=conf)\n",
        "\n",
        "sqlContext = SparkSession.builder \\\n",
        "    .master(\"local\") \\\n",
        "    .appName(\"SEC_Filings_Processing\") \\\n",
        "    .config('spark.ui.port', '4050') \\\n",
        "    .getOrCreate()\n",
        "\n",
        "print(f\"Spark Version: {sqlContext.sparkContext.version}\")\n",
        "print(\"SparkSession initialized successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o2tDrmzHmJCy",
        "outputId": "d3b17f91-1921-4094-f43e-73b2ea4a7bcb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spark Version: 3.4.1\n",
            "SparkSession initialized successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def ingest_file(file_path, sql_context):\n",
        "    try:\n",
        "        df = sql_context.read.option(\"header\", \"true\") \\\n",
        "            .option(\"inferSchema\", \"true\") \\\n",
        "            .option(\"quote\", \"\\\"\") \\\n",
        "            .option(\"escape\", \"\\\"\") \\\n",
        "            .csv(file_path)\n",
        "        print(f\"Successfully ingested {file_path}. Total rows: {df.count()}\")\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        print(f\"Error ingesting CSV: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "input_path = \"SEC_filings.csv\"\n",
        "\n",
        "df_without_changing = ingest_file(input_path, sqlContext)\n",
        "\n",
        "print(\"Sample of raw data:\")\n",
        "df_without_changing.show(truncate=False)\n",
        "print(\"Schema of raw data:\")\n",
        "df_without_changing.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fqhy646JmLfM",
        "outputId": "fbdaeae3-7589-48cf-f5f3-aea8a1d389b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully ingested SEC_filings.csv. Total rows: 2622\n",
            "Sample of raw data:\n",
            "+------+------------------------------+----------------------+-----------+--------------+----------+--------------+------------+------+-------------+----------------------------+--------+-------+---------------+------------+\n",
            "|Ticker|Name                          |Sector                |Asset Class|Market Value  |Weight (%)|Notional Value|Quantity    |Price |Location     |Exchange                    |Currency|FX Rate|Market Currency|Accrual Date|\n",
            "+------+------------------------------+----------------------+-----------+--------------+----------+--------------+------------+------+-------------+----------------------------+--------+-------+---------------+------------+\n",
            "|AAPL  |APPLE INC                     |Information Technology|Equity     |559,365,151.11|5.16      |559,365,151.11|4,305,127.00|129.93|United States|NASDAQ                      |USD     |1      |USD            |-           |\n",
            "|MSFT  |MICROSOFT CORP                |Information Technology|Equity     |513,917,712.42|4.74      |513,917,712.42|2,142,931.00|239.82|United States|NASDAQ                      |USD     |1      |USD            |-           |\n",
            "|AMZN  |AMAZON COM INC                |Consumer Discretionary|Equity     |213,823,596.00|1.97      |213,823,596.00|2,545,519.00|84    |United States|NASDAQ                      |USD     |1      |USD            |-           |\n",
            "|BRKB  |BERKSHIRE HATHAWAY INC CLASS B|Financials            |Equity     |159,603,687.60|1.47      |159,603,687.60|516,684.00  |308.9 |United States|New York Stock Exchange Inc.|USD     |1      |USD            |-           |\n",
            "|GOOGL |ALPHABET INC CLASS A          |Communication         |Equity     |151,996,026.75|1.4       |151,996,026.75|1,722,725.00|88.23 |United States|NASDAQ                      |USD     |1      |USD            |-           |\n",
            "|UNH   |UNITEDHEALTH GROUP INC        |Health Care           |Equity     |142,028,859.84|1.31      |142,028,859.84|267,888.00  |530.18|United States|New York Stock Exchange Inc.|USD     |1      |USD            |-           |\n",
            "|GOOG  |ALPHABET INC CLASS C          |Communication         |Equity     |135,557,878.61|1.25      |135,557,878.61|1,527,757.00|88.73 |United States|NASDAQ                      |USD     |1      |USD            |-           |\n",
            "|JNJ   |JOHNSON & JOHNSON             |Health Care           |Equity     |133,147,817.70|1.23      |133,147,817.70|753,738.00  |176.65|United States|New York Stock Exchange Inc.|USD     |1      |USD            |-           |\n",
            "|XOM   |EXXON MOBIL CORP              |Energy                |Equity     |130,179,148.40|1.2       |130,179,148.40|1,180,228.00|110.3 |United States|New York Stock Exchange Inc.|USD     |1      |USD            |-           |\n",
            "|JPM   |JPMORGAN CHASE & CO           |Financials            |Equity     |112,022,178.30|1.03      |112,022,178.30|835,363.00  |134.1 |United States|New York Stock Exchange Inc.|USD     |1      |USD            |-           |\n",
            "|PG    |PROCTER & GAMBLE              |Consumer Staples      |Equity     |102,458,349.00|0.95      |102,458,349.00|676,025.00  |151.56|United States|New York Stock Exchange Inc.|USD     |1      |USD            |-           |\n",
            "|NVDA  |NVIDIA CORP                   |Information Technology|Equity     |100,792,173.44|0.93      |100,792,173.44|689,696.00  |146.14|United States|NASDAQ                      |USD     |1      |USD            |-           |\n",
            "|CVX   |CHEVRON CORP                  |Energy                |Equity     |99,417,895.59 |0.92      |99,417,895.59 |553,891.00  |179.49|United States|New York Stock Exchange Inc.|USD     |1      |USD            |-           |\n",
            "|V     |VISA INC CLASS A              |Information Technology|Equity     |97,955,931.36 |0.9       |97,955,931.36 |471,486.00  |207.76|United States|New York Stock Exchange Inc.|USD     |1      |USD            |-           |\n",
            "|HD    |HOME DEPOT INC                |Consumer Discretionary|Equity     |93,465,816.74 |0.86      |93,465,816.74 |295,909.00  |315.86|United States|New York Stock Exchange Inc.|USD     |1      |USD            |-           |\n",
            "|TSLA  |TESLA INC                     |Consumer Discretionary|Equity     |91,176,481.02 |0.84      |91,176,481.02 |740,189.00  |123.18|United States|NASDAQ                      |USD     |1      |USD            |-           |\n",
            "|LLY   |ELI LILLY                     |Health Care           |Equity     |88,285,240.48 |0.82      |88,285,240.48 |241,322.00  |365.84|United States|New York Stock Exchange Inc.|USD     |1      |USD            |-           |\n",
            "|MA    |MASTERCARD INC CLASS A        |Information Technology|Equity     |84,738,671.43 |0.78      |84,738,671.43 |243,691.00  |347.73|United States|New York Stock Exchange Inc.|USD     |1      |USD            |-           |\n",
            "|PFE   |PFIZER INC                    |Health Care           |Equity     |82,940,804.52 |0.77      |82,940,804.52 |1,618,673.00|51.24 |United States|New York Stock Exchange Inc.|USD     |1      |USD            |-           |\n",
            "|ABBV  |ABBVIE INC                    |Health Care           |Equity     |81,797,931.84 |0.76      |81,797,931.84 |506,144.00  |161.61|United States|New York Stock Exchange Inc.|USD     |1      |USD            |-           |\n",
            "+------+------------------------------+----------------------+-----------+--------------+----------+--------------+------------+------+-------------+----------------------------+--------+-------+---------------+------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "Schema of raw data:\n",
            "root\n",
            " |-- Ticker: string (nullable = true)\n",
            " |-- Name: string (nullable = true)\n",
            " |-- Sector: string (nullable = true)\n",
            " |-- Asset Class: string (nullable = true)\n",
            " |-- Market Value: string (nullable = true)\n",
            " |-- Weight (%): string (nullable = true)\n",
            " |-- Notional Value: string (nullable = true)\n",
            " |-- Quantity: string (nullable = true)\n",
            " |-- Price: string (nullable = true)\n",
            " |-- Location: string (nullable = true)\n",
            " |-- Exchange: string (nullable = true)\n",
            " |-- Currency: string (nullable = true)\n",
            " |-- FX Rate: string (nullable = true)\n",
            " |-- Market Currency: string (nullable = true)\n",
            " |-- Accrual Date: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, lower, regexp_replace, trim, when, isnull\n",
        "from pyspark.sql.types import FloatType, IntegerType\n",
        "\n",
        "def data_cleaning(df):\n",
        "    try:\n",
        "        cleaning_df = df.withColumn(\"Name\", lower(trim(col(\"Name\")))) \\\n",
        "            .withColumn(\"Sector\", lower(trim(col(\"Sector\")))) \\\n",
        "            .withColumn(\"Ticker\", lower(trim(col(\"Ticker\"))))\n",
        "\n",
        "        cleaning_df = cleaning_df.withColumn(\"Name\",\n",
        "            regexp_replace(col(\"Name\"), \"<[^>]+>|[^a-zA-Z0-9\\\\s]\", \"\"))\n",
        "\n",
        "        cleaning_df = cleaning_df.withColumn(\"Sector\",\n",
        "            when(isnull(col(\"Sector\")), \"unknown\").otherwise(col(\"Sector\"))) \\\n",
        "            .withColumn(\"Market_Value\",\n",
        "                when(isnull(col(\"Market Value\")), 0.0).otherwise(col(\"Market Value\"))) \\\n",
        "            .withColumn(\"Weight\",\n",
        "                when(isnull(col(\"Weight (%)\")), 0.0).otherwise(col(\"Weight (%)\")))\n",
        "\n",
        "        cleaning_df = cleaning_df.dropDuplicates([\"Ticker\", \"Name\"])\n",
        "\n",
        "        cleaning_df = cleaning_df.withColumn(\"Market_Value\", col(\"Market_Value\").cast(FloatType())) \\\n",
        "            .withColumn(\"Weight\", col(\"Weight\").cast(FloatType())) \\\n",
        "            .withColumn(\"Quantity\", col(\"Quantity\").cast(IntegerType())) \\\n",
        "            .withColumn(\"Price\", col(\"Price\").cast(FloatType()))\n",
        "\n",
        "        print(f\"Data cleaned. Total rows after cleaning: {cleaning_df.count()}\")\n",
        "        return cleaning_df\n",
        "    except Exception as e:\n",
        "        print(f\"Error cleaning data: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "cleaned_df = data_cleaning(df_without_changing)\n",
        "\n",
        "print(\"Sample of cleaned data:\")\n",
        "cleaned_df.show(truncate=False)\n",
        "print(\"Schema of cleaned data:\")\n",
        "cleaned_df.printSchema()"
      ],
      "metadata": {
        "id": "sEFvhsnfmJvN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6908648-bc9b-4b2e-a14a-ff03bb98d6b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data cleaned. Total rows after cleaning: 2622\n",
            "Sample of cleaned data:\n",
            "+------+--------------------------------+----------------------+-----------+--------------+----------+--------------+--------+------+-------------+----------------------------+--------+-------+---------------+------------+------------+------+\n",
            "|Ticker|Name                            |Sector                |Asset Class|Market Value  |Weight (%)|Notional Value|Quantity|Price |Location     |Exchange                    |Currency|FX Rate|Market Currency|Accrual Date|Market_Value|Weight|\n",
            "+------+--------------------------------+----------------------+-----------+--------------+----------+--------------+--------+------+-------------+----------------------------+--------+-------+---------------+------------+------------+------+\n",
            "|-     |omniab inc 1250 vesting prvt    |health care           |Equity     |0.02          |0         |0.02          |null    |0.0   |United States|NO MARKET (E.G. UNLISTED)   |USD     |1      |USD            |-           |0.02        |0.0   |\n",
            "|-     |omniab inc 1500 vesting prvt    |health care           |Equity     |0.02          |0         |0.02          |null    |0.0   |United States|NO MARKET (E.G. UNLISTED)   |USD     |1      |USD            |-           |0.02        |0.0   |\n",
            "|a     |agilent technologies inc        |health care           |Equity     |12,728,480.75 |0.12      |12,728,480.75 |null    |149.65|United States|New York Stock Exchange Inc.|USD     |1      |USD            |-           |null        |0.12  |\n",
            "|aa    |alcoa corp                      |materials             |Equity     |2,384,992.44  |0.02      |2,384,992.44  |null    |45.47 |United States|New York Stock Exchange Inc.|USD     |1      |USD            |-           |null        |0.02  |\n",
            "|aal   |american airlines group inc     |industrials           |Equity     |2,396,638.80  |0.02      |2,396,638.80  |null    |12.72 |United States|NASDAQ                      |USD     |1      |USD            |-           |null        |0.02  |\n",
            "|aan   |aarons company inc              |consumer discretionary|Equity     |117,779.20    |0         |117,779.20    |null    |11.95 |United States|New York Stock Exchange Inc.|USD     |1      |USD            |-           |null        |0.0   |\n",
            "|aaon  |aaon inc                        |industrials           |Equity     |860,305.04    |0.01      |860,305.04    |null    |75.32 |United States|NASDAQ                      |USD     |1      |USD            |-           |null        |0.01  |\n",
            "|aap   |advance auto parts inc          |consumer discretionary|Equity     |2,611,105.77  |0.02      |2,611,105.77  |null    |147.03|United States|New York Stock Exchange Inc.|USD     |1      |USD            |-           |null        |0.02  |\n",
            "|aapl  |apple inc                       |information technology|Equity     |559,365,151.11|5.16      |559,365,151.11|null    |129.93|United States|NASDAQ                      |USD     |1      |USD            |-           |null        |5.16  |\n",
            "|aat   |american assets trust reit inc  |real estate           |Equity     |355,815.50    |0         |355,815.50    |null    |26.5  |United States|New York Stock Exchange Inc.|USD     |1      |USD            |Jan 01, 1970|null        |0.0   |\n",
            "|aaww  |atlas air worldwide holdings inc|industrials           |Equity     |866,275.20    |0.01      |866,275.20    |null    |100.8 |United States|NASDAQ                      |USD     |1      |USD            |-           |null        |0.01  |\n",
            "|abbv  |abbvie inc                      |health care           |Equity     |81,797,931.84 |0.76      |81,797,931.84 |null    |161.61|United States|New York Stock Exchange Inc.|USD     |1      |USD            |-           |null        |0.76  |\n",
            "|abc   |amerisourcebergen corp          |health care           |Equity     |7,340,124.45  |0.07      |7,340,124.45  |null    |165.71|United States|New York Stock Exchange Inc.|USD     |1      |USD            |-           |null        |0.07  |\n",
            "|abcb  |ameris bancorp                  |financials            |Equity     |769,419.08    |0.01      |769,419.08    |null    |47.14 |United States|NASDAQ                      |USD     |1      |USD            |-           |null        |0.01  |\n",
            "|abcl  |abcellera biologics inc         |health care           |Equity     |634,360.86    |0.01      |634,360.86    |null    |10.13 |Canada       |NASDAQ                      |USD     |1      |USD            |-           |null        |0.01  |\n",
            "|abg   |asbury automotive group inc     |consumer discretionary|Equity     |1,152,577.50  |0.01      |1,152,577.50  |null    |179.25|United States|New York Stock Exchange Inc.|USD     |1      |USD            |-           |null        |0.01  |\n",
            "|abm   |abm industries inc              |industrials           |Equity     |839,982.20    |0.01      |839,982.20    |null    |44.42 |United States|New York Stock Exchange Inc.|USD     |1      |USD            |-           |null        |0.01  |\n",
            "|abnb  |airbnb inc class a              |consumer discretionary|Equity     |9,183,811.50  |0.08      |9,183,811.50  |null    |85.5  |United States|NASDAQ                      |USD     |1      |USD            |-           |null        |0.08  |\n",
            "|abr   |arbor realty trust reit inc     |financials            |Equity     |593,365.34    |0.01      |593,365.34    |null    |13.19 |United States|New York Stock Exchange Inc.|USD     |1      |USD            |-           |null        |0.01  |\n",
            "|abt   |abbott laboratories             |health care           |Equity     |53,916,551.52 |0.5       |53,916,551.52 |null    |109.79|United States|New York Stock Exchange Inc.|USD     |1      |USD            |-           |null        |0.5   |\n",
            "+------+--------------------------------+----------------------+-----------+--------------+----------+--------------+--------+------+-------------+----------------------------+--------+-------+---------------+------------+------------+------+\n",
            "only showing top 20 rows\n",
            "\n",
            "Schema of cleaned data:\n",
            "root\n",
            " |-- Ticker: string (nullable = true)\n",
            " |-- Name: string (nullable = true)\n",
            " |-- Sector: string (nullable = true)\n",
            " |-- Asset Class: string (nullable = true)\n",
            " |-- Market Value: string (nullable = true)\n",
            " |-- Weight (%): string (nullable = true)\n",
            " |-- Notional Value: string (nullable = true)\n",
            " |-- Quantity: integer (nullable = true)\n",
            " |-- Price: float (nullable = true)\n",
            " |-- Location: string (nullable = true)\n",
            " |-- Exchange: string (nullable = true)\n",
            " |-- Currency: string (nullable = true)\n",
            " |-- FX Rate: string (nullable = true)\n",
            " |-- Market Currency: string (nullable = true)\n",
            " |-- Accrual Date: string (nullable = true)\n",
            " |-- Market_Value: float (nullable = true)\n",
            " |-- Weight: float (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import StringType\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "def metadata_extraction(input_df):\n",
        "    try:\n",
        "        extracted_metadata = input_df.select(\n",
        "            col(\"Ticker\").alias(\"ticker\"),\n",
        "            col(\"Name\").alias(\"company_name\"),\n",
        "            col(\"Sector\").alias(\"sector\"),\n",
        "            col(\"Market_Value\").alias(\"market_value\"),\n",
        "            col(\"Weight\").alias(\"weight_percent\"),\n",
        "            col(\"Quantity\").alias(\"shares_quantity\"),\n",
        "            col(\"Price\").alias(\"share_price\"),\n",
        "            col(\"Location\").alias(\"country\"),\n",
        "            col(\"Exchange\").alias(\"stock_exchange\"),\n",
        "            col(\"Accrual Date\").alias(\"accrual_date\")\n",
        "        )\n",
        "        extracted_metadata = extracted_metadata.withColumn(\"record_id\",\n",
        "            col(\"ticker\").cast(StringType()) + \"_\" + col(\"company_name\").cast(StringType()))\n",
        "        print(\"Metadata extracted successfully.\")\n",
        "        return extracted_metadata\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting metadata: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "extracted_metadata = metadata_extraction(cleaned_df)\n",
        "\n",
        "print(\"Sample of metadata:\")\n",
        "extracted_metadata.show(truncate=False)\n",
        "print(\"Schema of metadata:\")\n",
        "extracted_metadata.printSchema()"
      ],
      "metadata": {
        "id": "rhHf5c48mTDY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8048ced0-e181-4edc-9239-f2a9d0e43d5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Metadata extracted successfully.\n",
            "Sample of metadata:\n",
            "+------+--------------------------------+----------------------+------------+--------------+---------------+-----------+-------------+----------------------------+------------+---------+\n",
            "|ticker|company_name                    |sector                |market_value|weight_percent|shares_quantity|share_price|country      |stock_exchange              |accrual_date|record_id|\n",
            "+------+--------------------------------+----------------------+------------+--------------+---------------+-----------+-------------+----------------------------+------------+---------+\n",
            "|-     |omniab inc 1250 vesting prvt    |health care           |0.02        |0.0           |null           |0.0        |United States|NO MARKET (E.G. UNLISTED)   |-           |null     |\n",
            "|-     |omniab inc 1500 vesting prvt    |health care           |0.02        |0.0           |null           |0.0        |United States|NO MARKET (E.G. UNLISTED)   |-           |null     |\n",
            "|a     |agilent technologies inc        |health care           |null        |0.12          |null           |149.65     |United States|New York Stock Exchange Inc.|-           |null     |\n",
            "|aa    |alcoa corp                      |materials             |null        |0.02          |null           |45.47      |United States|New York Stock Exchange Inc.|-           |null     |\n",
            "|aal   |american airlines group inc     |industrials           |null        |0.02          |null           |12.72      |United States|NASDAQ                      |-           |null     |\n",
            "|aan   |aarons company inc              |consumer discretionary|null        |0.0           |null           |11.95      |United States|New York Stock Exchange Inc.|-           |null     |\n",
            "|aaon  |aaon inc                        |industrials           |null        |0.01          |null           |75.32      |United States|NASDAQ                      |-           |null     |\n",
            "|aap   |advance auto parts inc          |consumer discretionary|null        |0.02          |null           |147.03     |United States|New York Stock Exchange Inc.|-           |null     |\n",
            "|aapl  |apple inc                       |information technology|null        |5.16          |null           |129.93     |United States|NASDAQ                      |-           |null     |\n",
            "|aat   |american assets trust reit inc  |real estate           |null        |0.0           |null           |26.5       |United States|New York Stock Exchange Inc.|Jan 01, 1970|null     |\n",
            "|aaww  |atlas air worldwide holdings inc|industrials           |null        |0.01          |null           |100.8      |United States|NASDAQ                      |-           |null     |\n",
            "|abbv  |abbvie inc                      |health care           |null        |0.76          |null           |161.61     |United States|New York Stock Exchange Inc.|-           |null     |\n",
            "|abc   |amerisourcebergen corp          |health care           |null        |0.07          |null           |165.71     |United States|New York Stock Exchange Inc.|-           |null     |\n",
            "|abcb  |ameris bancorp                  |financials            |null        |0.01          |null           |47.14      |United States|NASDAQ                      |-           |null     |\n",
            "|abcl  |abcellera biologics inc         |health care           |null        |0.01          |null           |10.13      |Canada       |NASDAQ                      |-           |null     |\n",
            "|abg   |asbury automotive group inc     |consumer discretionary|null        |0.01          |null           |179.25     |United States|New York Stock Exchange Inc.|-           |null     |\n",
            "|abm   |abm industries inc              |industrials           |null        |0.01          |null           |44.42      |United States|New York Stock Exchange Inc.|-           |null     |\n",
            "|abnb  |airbnb inc class a              |consumer discretionary|null        |0.08          |null           |85.5       |United States|NASDAQ                      |-           |null     |\n",
            "|abr   |arbor realty trust reit inc     |financials            |null        |0.01          |null           |13.19      |United States|New York Stock Exchange Inc.|-           |null     |\n",
            "|abt   |abbott laboratories             |health care           |null        |0.5           |null           |109.79     |United States|New York Stock Exchange Inc.|-           |null     |\n",
            "+------+--------------------------------+----------------------+------------+--------------+---------------+-----------+-------------+----------------------------+------------+---------+\n",
            "only showing top 20 rows\n",
            "\n",
            "Schema of metadata:\n",
            "root\n",
            " |-- ticker: string (nullable = true)\n",
            " |-- company_name: string (nullable = true)\n",
            " |-- sector: string (nullable = true)\n",
            " |-- market_value: float (nullable = true)\n",
            " |-- weight_percent: float (nullable = true)\n",
            " |-- shares_quantity: integer (nullable = true)\n",
            " |-- share_price: float (nullable = true)\n",
            " |-- country: string (nullable = true)\n",
            " |-- stock_exchange: string (nullable = true)\n",
            " |-- accrual_date: string (nullable = true)\n",
            " |-- record_id: double (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def data_storing(df, output_path):\n",
        "    try:\n",
        "        df.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(output_path)\n",
        "        print(f\"Data successfully saved to {output_path}\")\n",
        "\n",
        "        df_storing = sqlContext.read.option(\"header\", \"true\").csv(output_path)\n",
        "        print(f\"Verification: Read back {df_storing.count()} rows from {output_path}\")\n",
        "        print(\"Sample of stored data:\")\n",
        "        df_storing.show(truncate=False)\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving data: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "output_path = \"processed_sec_filings\"\n",
        "\n",
        "data_storing(extracted_metadata, output_path)"
      ],
      "metadata": {
        "id": "2K5Hi6m9mU9a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30c9c22b-63c5-4950-aae5-32efeb88886b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data successfully saved to processed_sec_filings\n",
            "Verification: Read back 2622 rows from processed_sec_filings\n",
            "Sample of stored data:\n",
            "+------+--------------------------------+----------------------+------------+--------------+---------------+-----------+-------------+----------------------------+------------+---------+\n",
            "|ticker|company_name                    |sector                |market_value|weight_percent|shares_quantity|share_price|country      |stock_exchange              |accrual_date|record_id|\n",
            "+------+--------------------------------+----------------------+------------+--------------+---------------+-----------+-------------+----------------------------+------------+---------+\n",
            "|-     |omniab inc 1250 vesting prvt    |health care           |0.02        |0.0           |null           |0.0        |United States|NO MARKET (E.G. UNLISTED)   |-           |null     |\n",
            "|-     |omniab inc 1500 vesting prvt    |health care           |0.02        |0.0           |null           |0.0        |United States|NO MARKET (E.G. UNLISTED)   |-           |null     |\n",
            "|a     |agilent technologies inc        |health care           |null        |0.12          |null           |149.65     |United States|New York Stock Exchange Inc.|-           |null     |\n",
            "|aa    |alcoa corp                      |materials             |null        |0.02          |null           |45.47      |United States|New York Stock Exchange Inc.|-           |null     |\n",
            "|aal   |american airlines group inc     |industrials           |null        |0.02          |null           |12.72      |United States|NASDAQ                      |-           |null     |\n",
            "|aan   |aarons company inc              |consumer discretionary|null        |0.0           |null           |11.95      |United States|New York Stock Exchange Inc.|-           |null     |\n",
            "|aaon  |aaon inc                        |industrials           |null        |0.01          |null           |75.32      |United States|NASDAQ                      |-           |null     |\n",
            "|aap   |advance auto parts inc          |consumer discretionary|null        |0.02          |null           |147.03     |United States|New York Stock Exchange Inc.|-           |null     |\n",
            "|aapl  |apple inc                       |information technology|null        |5.16          |null           |129.93     |United States|NASDAQ                      |-           |null     |\n",
            "|aat   |american assets trust reit inc  |real estate           |null        |0.0           |null           |26.5       |United States|New York Stock Exchange Inc.|Jan 01, 1970|null     |\n",
            "|aaww  |atlas air worldwide holdings inc|industrials           |null        |0.01          |null           |100.8      |United States|NASDAQ                      |-           |null     |\n",
            "|abbv  |abbvie inc                      |health care           |null        |0.76          |null           |161.61     |United States|New York Stock Exchange Inc.|-           |null     |\n",
            "|abc   |amerisourcebergen corp          |health care           |null        |0.07          |null           |165.71     |United States|New York Stock Exchange Inc.|-           |null     |\n",
            "|abcb  |ameris bancorp                  |financials            |null        |0.01          |null           |47.14      |United States|NASDAQ                      |-           |null     |\n",
            "|abcl  |abcellera biologics inc         |health care           |null        |0.01          |null           |10.13      |Canada       |NASDAQ                      |-           |null     |\n",
            "|abg   |asbury automotive group inc     |consumer discretionary|null        |0.01          |null           |179.25     |United States|New York Stock Exchange Inc.|-           |null     |\n",
            "|abm   |abm industries inc              |industrials           |null        |0.01          |null           |44.42      |United States|New York Stock Exchange Inc.|-           |null     |\n",
            "|abnb  |airbnb inc class a              |consumer discretionary|null        |0.08          |null           |85.5       |United States|NASDAQ                      |-           |null     |\n",
            "|abr   |arbor realty trust reit inc     |financials            |null        |0.01          |null           |13.19      |United States|New York Stock Exchange Inc.|-           |null     |\n",
            "|abt   |abbott laboratories             |health care           |null        |0.5           |null           |109.79     |United States|New York Stock Exchange Inc.|-           |null     |\n",
            "+------+--------------------------------+----------------------+------------+--------------+---------------+-----------+-------------+----------------------------+------------+---------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q pyspark==3.4.1 spark-nlp==5.1.3\n",
        "!pip install faiss-cpu"
      ],
      "metadata": {
        "id": "raFOEwde-0dW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42782725-f4b6-454e-cfa0-c57fd8f18b6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/dist-packages (1.10.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark_session = SparkSession.getActiveSession()\n",
        "\n",
        "if spark_session:\n",
        "    spark_session.stop()"
      ],
      "metadata": {
        "id": "KaPXh95SGeP-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C4q0rmyJ36Un"
      },
      "outputs": [],
      "source": [
        "import sparknlp\n",
        "spark = sparknlp.start()\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark import SparkContext, SparkConf\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sparknlp.pretrained import ResourceDownloader, PretrainedPipeline\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import faiss\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = spark.read.json(\"/content/data/CNBC_financial_news_1.json\")\n",
        "df2 = spark.read.json(\"/content/data/CNBC_financial_articles_2.json\")\n",
        "cnbc_df = df1.union(df2)\n",
        "\n",
        "reddit_df = spark.read.json(\"/content/data/reddit_posts.json\")"
      ],
      "metadata": {
        "id": "HX-BJ-BWuzoO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(df, col_name):\n",
        "    return df.withColumn(col_name, lower(col(col_name))) \\\n",
        "             .withColumn(col_name, regexp_replace(col(col_name), \"<.*?>\", \"\")) \\\n",
        "             .withColumn(col_name, regexp_replace(col(col_name), \"[^a-zA-Z0-9\\\\s]\", \"\"))\n",
        "\n",
        "cnbc_df = clean_text(cnbc_df, \"content\")\n",
        "reddit_df = clean_text(reddit_df, \"Title\")"
      ],
      "metadata": {
        "id": "xNviU6qevkhp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ner_pipeline = PretrainedPipeline(\"recognize_entities_dl\", lang=\"en\", remote_loc=None)\n",
        "\n",
        "sample_text = cnbc_df.select(\"content\").limit(1).collect()[0][0]\n",
        "ner_result = ner_pipeline.fullAnnotate(sample_text)\n",
        "print(ner_result[0][\"entities\"])"
      ],
      "metadata": {
        "id": "2ZGI1L8dv0V8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33aca178-8f4d-4c13-d95c-595f7f5c1eee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "recognize_entities_dl download started this may take some time.\n",
            "Approx size to download 159 MB\n",
            "[OK!]\n",
            "[Annotation(chunk, 3148, 3154, severin, {'entity': 'PER', 'sentence': '0', 'chunk': '0'}, [])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "\n",
        "cnbc_pd = cnbc_df.select(\"title\", \"content\", \"date\", \"url\").toPandas()\n",
        "reddit_pd = reddit_df.select(\"Title\", \"URL\", \"Post_Time\").toPandas()\n",
        "reddit_pd.rename(columns={\"Title\": \"title\", \"URL\": \"url\", \"Post_Time\": \"date\"}, inplace=True)\n",
        "\n",
        "combined = pd.concat([cnbc_pd, reddit_pd], ignore_index=True)\n",
        "\n",
        "combined['content'] = combined['content'].fillna('')\n",
        "\n",
        "combined[\"text\"] = combined[\"title\"] + \" \" + combined[\"content\"]\n",
        "\n",
        "combined['text'] = combined['text'].astype(str)\n",
        "\n",
        "combined[\"embedding\"] = combined[\"text\"].apply(lambda x: model.encode(x).tolist())"
      ],
      "metadata": {
        "id": "LKhVHPP2xFXt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f6ea551-3ae3-4bf1-e73c-76b19ff9d948"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dimension = len(combined[\"embedding\"][0])\n",
        "index = faiss.IndexFlatL2(dimension)\n",
        "\n",
        "embedding_matrix = np.vstack(combined[\"embedding\"].values)\n",
        "index.add(embedding_matrix)"
      ],
      "metadata": {
        "id": "tHuvtwXA1QbJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def search_similar(query, k=3):\n",
        "    q_embed = model.encode([query])\n",
        "    distances, indices = index.search(np.array(q_embed), k)\n",
        "    return combined.iloc[indices[0]][[\"title\", \"text\", \"url\"]]\n",
        "\n",
        "results = search_similar(\"What’s the impact of Trump’s tariffs on inflation?\")\n",
        "print(results)\n"
      ],
      "metadata": {
        "id": "JMKWrXsO3zzX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db584f92-1b24-406b-afad-5b6b2f12fca6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                 title  \\\n",
            "465           trumponomics big tariffs bigger deficits   \n",
            "136  powell indicates tariffs could pose a challeng...   \n",
            "24   Powell indicates tariffs could pose a challeng...   \n",
            "\n",
            "                                                  text  \\\n",
            "465          trumponomics big tariffs bigger deficits    \n",
            "136  powell indicates tariffs could pose a challeng...   \n",
            "24   Powell indicates tariffs could pose a challeng...   \n",
            "\n",
            "                                                   url  \n",
            "465             https://www.reddit.com/gallery/1juq59q  \n",
            "136  https://www.cnbc.com/2025/04/16/powell-indicat...  \n",
            "24   https://www.cnbc.com/2025/04/16/powell-indicat...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = search_similar(\"Apple is acquiring a startup in New York.\")\n",
        "print(results)"
      ],
      "metadata": {
        "id": "6ebO_Wb14Wmg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "899c496a-f203-4cb3-f214-453e7d5a8e0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                 title  \\\n",
            "293                                          bad apple   \n",
            "7    Stocks making the biggest moves midday: Apple,...   \n",
            "517  apple redirects iphone production to india ami...   \n",
            "\n",
            "                                                  text  \\\n",
            "293                                         bad apple    \n",
            "7    Stocks making the biggest moves midday: Apple,...   \n",
            "517  apple redirects iphone production to india ami...   \n",
            "\n",
            "                                                   url  \n",
            "293                    https://v.redd.it/cqp9noh9y3ue1  \n",
            "7    https://www.cnbc.com/2025/04/14/stocks-making-...  \n",
            "517  https://www.reddit.com/r/wallstreetbets/commen...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "file_paths = [\n",
        "    \"/content/data/reddit_posts.json\",\n",
        "    \"/content/data/CNBC_financial_news_1.json\",\n",
        "    \"/content/data/CNBC_financial_articles_2.json\"\n",
        "]\n",
        "\n",
        "all_documents = []\n",
        "\n",
        "for file_path in file_paths:\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if line:\n",
        "                try:\n",
        "                    all_documents.append(json.loads(line))\n",
        "                except json.JSONDecodeError:\n",
        "                    continue"
      ],
      "metadata": {
        "id": "W8aPFM4oXWNt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "import faiss\n",
        "\n",
        "processed_docs = []\n",
        "text_fields = [\"Title\", \"Content\", \"Summary\", \"Text\", \"text\", \"headline\", \"body\"]\n",
        "\n",
        "for doc in all_documents:\n",
        "    text = \"\"\n",
        "    for field in text_fields:\n",
        "        if field in doc and isinstance(doc[field], str):\n",
        "            text += doc[field].strip() + \" \"\n",
        "    text = text.strip()\n",
        "    if text:\n",
        "        processed_docs.append({\"text\": text})\n",
        "\n",
        "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "corpus = [doc[\"text\"] for doc in processed_docs]\n",
        "embeddings = model.encode(corpus, convert_to_numpy=True).astype(\"float32\")\n",
        "\n",
        "dimension = embeddings.shape[1]\n",
        "faiss_index = faiss.IndexFlatIP(dimension)\n",
        "faiss_index.add(embeddings)\n"
      ],
      "metadata": {
        "id": "yi6jP2FTXCS8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import faiss\n",
        "\n",
        "faiss.write_index(faiss_index, \"faiss_index.bin\")\n",
        "\n",
        "with open(\"processed_docs.pkl\", \"wb\") as f:\n",
        "    pickle.dump(processed_docs, f)"
      ],
      "metadata": {
        "id": "VyS2onp1VImh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-community"
      ],
      "metadata": {
        "id": "Vt-VnhbW45FD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98f01aff-add5-4369-bebc-88364de94c0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.11/dist-packages (0.3.21)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.51 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.52)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.23 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.23)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.40)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.11.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (9.1.2)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.9.1)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.31)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.4.0)\n",
            "Requirement already satisfied: numpy<3,>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.19.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.23->langchain-community) (0.3.8)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.23->langchain-community) (2.11.3)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.51->langchain-community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.51->langchain-community) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.51->langchain-community) (4.13.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (3.10.16)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.23.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.1.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2025.1.31)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.2.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.0.8)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.51->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.23->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.23->langchain-community) (2.33.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.3.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_community.llms import HuggingFacePipeline\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
        "\n",
        "def load_llm():\n",
        "    model_name = \"google/flan-t5-base\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "    pipe = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=200)\n",
        "    llm = HuggingFacePipeline(pipeline=pipe, model_kwargs={\"temperature\": 0.7})\n",
        "    return llm\n",
        "\n",
        "def run_rag(query: str, faiss_index, documents, embed_model, k=7):\n",
        "    query_vec = embed_model.encode(query).astype('float32').reshape(1, -1)\n",
        "\n",
        "    D, I = faiss_index.search(query_vec, k)\n",
        "    retrieved = [documents[i][\"text\"][:1000] for i in I[0] if i < len(documents)]\n",
        "    context = \"\\n\\n\".join(retrieved)\n",
        "\n",
        "    prompt = PromptTemplate(\n",
        "        input_variables=[\"context\", \"question\"],\n",
        "       template=\"\"\"\n",
        "Use the following financial information to answer the user's question in a factual and complete way. Use numbered citations if helpful.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question:\n",
        "{question}\n",
        "\n",
        "Answer (with sources if relevant):\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "    )\n",
        "\n",
        "    llm = load_llm()\n",
        "    rag_chain = prompt | llm\n",
        "    result = rag_chain.invoke({\"context\": context, \"question\": query})\n",
        "\n",
        "    print(\"📘 Answer:\\n\", result)\n",
        "    return result"
      ],
      "metadata": {
        "id": "s8WZ7J5iVnGc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What are the risks of Amazon’s $15B warehouse expansion?\"\n",
        "run_rag(query, faiss_index, processed_docs, model)"
      ],
      "metadata": {
        "id": "S5EFyZtNXtIB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 167
        },
        "outputId": "a15750e0-9695-4bdf-f2db-0f01d62768c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "<ipython-input-40-232144c2a995>:11: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n",
            "  llm = HuggingFacePipeline(pipeline=pipe, model_kwargs={\"temperature\": 0.7})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📘 Answer:\n",
            " Amazon Cancels Inventory Orders From China After Tariffs ECB cuts rates again to help economy weather erratic U.S. trade policy Is the market more fucked than it seems? DHL suspends shipments to the US with a value exceeding $800 (except B2B) China Orders Halts to Boeing Jet Deliveries as Trade War Expands\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Amazon Cancels Inventory Orders From China After Tariffs ECB cuts rates again to help economy weather erratic U.S. trade policy Is the market more fucked than it seems? DHL suspends shipments to the US with a value exceeding $800 (except B2B) China Orders Halts to Boeing Jet Deliveries as Trade War Expands'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install textstat"
      ],
      "metadata": {
        "id": "JiNpFihj6b3k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "424eb3b1-4f61-48c0-c49d-ac1d6d60099b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: textstat in /usr/local/lib/python3.11/dist-packages (0.7.5)\n",
            "Requirement already satisfied: pyphen in /usr/local/lib/python3.11/dist-packages (from textstat) (0.17.2)\n",
            "Requirement already satisfied: cmudict in /usr/local/lib/python3.11/dist-packages (from textstat) (1.0.32)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from textstat) (75.2.0)\n",
            "Requirement already satisfied: importlib-metadata>=5 in /usr/local/lib/python3.11/dist-packages (from cmudict->textstat) (8.6.1)\n",
            "Requirement already satisfied: importlib-resources>=5 in /usr/local/lib/python3.11/dist-packages (from cmudict->textstat) (6.5.2)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata>=5->cmudict->textstat) (3.21.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, lower, regexp_replace\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import faiss\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_community.llms import HuggingFacePipeline\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
        "import json"
      ],
      "metadata": {
        "id": "I-Wiae5J9lWp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(df, col_name):\n",
        "    return df.withColumn(col_name, lower(col(col_name))) \\\n",
        "             .withColumn(col_name, regexp_replace(col(col_name), \"<.*?>\", \"\")) \\\n",
        "             .withColumn(col_name, regexp_replace(col(col_name), \"[^a-zA-Z0-9\\\\s]\", \"\"))"
      ],
      "metadata": {
        "id": "gI_Jgm6E9sXQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = spark.read.json(\"/content/data/CNBC_financial_news_1.json\")\n",
        "df2 = spark.read.json(\"/content/data/CNBC_financial_articles_2.json\")\n",
        "cnbc_df = df1.union(df2)\n",
        "cnbc_df = clean_text(cnbc_df, \"content\")\n",
        "reddit_df = spark.read.json(\"/content/data/reddit_posts.json\")\n",
        "reddit_df = clean_text(reddit_df, \"Title\")"
      ],
      "metadata": {
        "id": "zIcA0aBE9uLH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnbc_pd = cnbc_df.select(\"title\", \"content\", \"date\", \"url\").toPandas()\n",
        "reddit_pd = reddit_df.select(\"Title\", \"URL\", \"Post_Time\").toPandas()\n",
        "reddit_pd.rename(columns={\"Title\": \"title\", \"URL\": \"url\", \"Post_Time\": \"date\"}, inplace=True)\n",
        "combined = pd.concat([cnbc_pd, reddit_pd], ignore_index=True)\n",
        "combined['content'] = combined['content'].fillna('')\n",
        "combined[\"text\"] = combined[\"title\"] + \" \" + combined[\"content\"]\n",
        "combined['text'] = combined['text'].astype(str)"
      ],
      "metadata": {
        "id": "ndlC6npI9wcP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "combined[\"embedding\"] = combined[\"text\"].apply(lambda x: model.encode(x).tolist())\n",
        "dimension = len(combined[\"embedding\"][0])\n",
        "index = faiss.IndexFlatL2(dimension)\n",
        "embedding_matrix = np.vstack(combined[\"embedding\"].values)\n",
        "index.add(embedding_matrix)\n"
      ],
      "metadata": {
        "id": "yE036pMn9xw6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_paths = [\"/content/data/reddit_posts.json\", \"/content/data/CNBC_financial_articles_2.json\", \"/content/data/CNBC_financial_news_1.json\"]\n",
        "all_documents = []\n",
        "for file_path in file_paths:\n",
        "    with open(file_path, \"r\") as f:\n",
        "        for line in f:\n",
        "            if line.strip():\n",
        "                try:\n",
        "                    all_documents.append(json.loads(line))\n",
        "                except json.JSONDecodeError:\n",
        "                    continue\n",
        "\n",
        "processed_docs = []\n",
        "text_fields = [\"Title\", \"Content\", \"Summary\", \"Text\", \"text\", \"headline\", \"body\"]\n",
        "for doc in all_documents:\n",
        "    text = \" \".join([doc[field].strip() for field in text_fields if field in doc and isinstance(doc[field], str)]).strip()\n",
        "    if text:\n",
        "        processed_docs.append({\"text\": text})\n",
        "\n",
        "corpus = [doc[\"text\"] for doc in processed_docs]\n",
        "embeddings = model.encode(corpus, convert_to_numpy=True).astype(\"float32\")\n",
        "dimension = embeddings.shape[1]\n",
        "faiss_index = faiss.IndexFlatIP(dimension)\n",
        "faiss_index.add(embeddings)"
      ],
      "metadata": {
        "id": "gBWT8z7U90kA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_llm():\n",
        "    model_name = \"google/flan-t5-base\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "    pipe = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=200)\n",
        "    return HuggingFacePipeline(pipeline=pipe, model_kwargs={\"temperature\": 0.7})\n",
        "\n",
        "def run_rag(query, faiss_index, documents, embed_model, k=7):\n",
        "    query_vec = embed_model.encode(query).astype('float32').reshape(1, -1)\n",
        "    D, I = faiss_index.search(query_vec, k)\n",
        "    retrieved = [documents[i][\"text\"][:1000] for i in I[0] if i < len(documents)]\n",
        "    context = \"\\n\\n\".join(retrieved)\n",
        "    prompt = PromptTemplate(\n",
        "        input_variables=[\"context\", \"question\"],\n",
        "        template=\"\"\"\n",
        "        Use the following financial information to answer the user's question clearly and factually. Cite sources by number. If context lacks specific information, state so.\n",
        "        Context: {context}\n",
        "        Question: {question}\n",
        "        Answer (with numbered citations):\n",
        "        \"\"\"\n",
        "    )\n",
        "    llm = load_llm()\n",
        "    rag_chain = prompt | llm\n",
        "    result = rag_chain.invoke({\"context\": context, \"question\": query})\n",
        "    return result, retrieved"
      ],
      "metadata": {
        "id": "wH0pht-f93Oh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_response(query, response, retrieved_docs):\n",
        "    evaluation = {\"query\": query, \"response\": response}\n",
        "    doc_texts = \" \".join(retrieved_docs).lower()\n",
        "    query_words = query.lower().split()\n",
        "    relevant_terms = [word for word in query_words if word in doc_texts]\n",
        "    accuracy_score = len(relevant_terms) / len(query_words)\n",
        "    evaluation[\"accuracy\"] = \"High\" if accuracy_score > 0.5 else \"Low\"\n",
        "    evaluation[\"accuracy_score\"] = accuracy_score\n",
        "\n",
        "    word_count = len(response.split())\n",
        "    sentence_count = response.count(\".\") + 1\n",
        "    evaluation[\"clarity\"] = \"High\" if word_count < 100 and sentence_count > 1 else \"Low\"\n",
        "    grounding_score = (0.5 if \"[1]\" in response else 0) + (0.5 if any(doc.lower()[:50] in response.lower() for doc in retrieved_docs) else 0)\n",
        "    evaluation[\"grounding\"] = \"High\" if grounding_score > 0.5 else \"Low\"\n",
        "    evaluation[\"grounding_score\"] = grounding_score\n",
        "    return evaluation"
      ],
      "metadata": {
        "id": "MDaoihzx95Fp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What are the risks of Amazon’s $15B warehouse expansion?\"\n",
        "response, retrieved_docs = run_rag(query, faiss_index, processed_docs, model)\n",
        "eval_result = evaluate_response(query, response, retrieved_docs)"
      ],
      "metadata": {
        "id": "nguWd8sR9-e9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbfa831a-cd65-434f-a32f-5f3a9fe408cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=== Checking Accuracy, Clarity, and Grounding ===\")\n",
        "print(f\"Query: {query}\")\n",
        "print(f\"Answer:\\n{response}\")\n",
        "print(f\"Retrieved Documents (Top 3):\\n{[doc[:100] + '...' for doc in retrieved_docs[:3]]}\")\n",
        "print(f\"Evaluation:\")\n",
        "print(f\"  - Accuracy: {eval_result['accuracy']} (Score: {eval_result['accuracy_score']:.2f})\")\n",
        "print(f\"  - Clarity: {eval_result['clarity']} (Words: {len(response.split())})\")\n",
        "print(f\"  - Grounding: {eval_result['grounding']} (Score: {eval_result['grounding_score']:.2f})\")\n",
        "\n"
      ],
      "metadata": {
        "id": "kPfUg5lAXur8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "900f9c42-dfa5-4290-9312-1a6db6455cd7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Checking Accuracy, Clarity, and Grounding ===\n",
            "Query: What are the risks of Amazon’s $15B warehouse expansion?\n",
            "Answer:\n",
            "I think there is still a huge downside risk for the US stock market, despite the exemption for electronics Amazon Cancels Inventory Orders From China After Tariffs ECB cuts rates again to help economy weather erratic U.S. trade policy\n",
            "Retrieved Documents (Top 3):\n",
            "['Amazon considers $15 billion warehouse expansion plan, Bloomberg News reports...', 'I think there is still a huge downside risk for the US stock market, despite the exemption for elect...', 'Amazon Cancels Inventory Orders From China After Tariffs...']\n",
            "Evaluation:\n",
            "  - Accuracy: Low (Score: 0.33)\n",
            "  - Clarity: High (Words: 39)\n",
            "  - Grounding: Low (Score: 0.50)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"llm_evaluation.json\", \"w\") as f:\n",
        "    json.dump(eval_result, f, indent=2)\n",
        "print(\"\\nEvaluation saved to llm_evaluation.json\")"
      ],
      "metadata": {
        "id": "SFvtI_bD-DBx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1ddb0a9-cc03-4afd-9489-bee9783ffae5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluation saved to llm_evaluation.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, lower, regexp_replace\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import faiss\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_community.llms import HuggingFacePipeline\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
        "from functools import lru_cache\n",
        "import time\n",
        "import json"
      ],
      "metadata": {
        "id": "ISdPA7tXAmrP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sparknlp\n",
        "spark = sparknlp.start()\n",
        "\n",
        "def clean_text(df, col_name):\n",
        "    return df.withColumn(col_name, lower(col(col_name))) \\\n",
        "             .withColumn(col_name, regexp_replace(col(col_name), \"<.*?>\", \"\")) \\\n",
        "             .withColumn(col_name, regexp_replace(col(col_name), \"[^a-zA-Z0-9\\\\s]\", \"\"))\n"
      ],
      "metadata": {
        "id": "abkURUYEAodD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "269e3b0a-9a10-47d0-f5c5-d1bb637e51dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning::Spark Session already created, some configs may not take.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = spark.read.json(\"/content/data/CNBC_financial_news_1.json\")\n",
        "df2 = spark.read.json(\"/content/data/CNBC_financial_articles_2.json\")\n",
        "cnbc_df = df1.union(df2)\n",
        "cnbc_df = clean_text(cnbc_df, \"content\")\n",
        "reddit_df = spark.read.json(\"/content/data/reddit_posts.json\")\n",
        "reddit_df = clean_text(reddit_df, \"Title\")\n",
        "cnbc_pd = cnbc_df.select(\"title\", \"content\", \"date\", \"url\").toPandas()\n",
        "reddit_pd = reddit_df.select(\"Title\", \"URL\", \"Post_Time\").toPandas()\n",
        "reddit_pd.rename(columns={\"Title\": \"title\", \"URL\": \"url\", \"Post_Time\": \"date\"}, inplace=True)\n",
        "combined = pd.concat([cnbc_pd, reddit_pd], ignore_index=True)\n",
        "combined['content'] = combined['content'].fillna('')\n",
        "combined[\"text\"] = combined[\"title\"] + \" \" + combined[\"content\"]\n",
        "combined['text'] = combined['text'].astype(str)"
      ],
      "metadata": {
        "id": "S170_uEJAp_w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "combined[\"embedding\"] = combined[\"text\"].apply(lambda x: model.encode(x).tolist())\n",
        "dimension = len(combined[\"embedding\"][0])\n",
        "index = faiss.IndexFlatL2(dimension)\n",
        "embedding_matrix = np.vstack(combined[\"embedding\"].values)\n",
        "index.add(embedding_matrix)"
      ],
      "metadata": {
        "id": "20YXUGssAryO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_paths = [\"/content/data/reddit_posts.json\", \"/content/data/CNBC_financial_articles_2.json\", \"/content/data/CNBC_financial_news_1.json\"]\n",
        "all_documents = []\n",
        "for file_path in file_paths:\n",
        "    with open(file_path, \"r\") as f:\n",
        "        for line in f:\n",
        "            if line.strip():\n",
        "                try:\n",
        "                    all_documents.append(json.loads(line))\n",
        "                except json.JSONDecodeError:\n",
        "                    continue\n",
        "processed_docs = []\n",
        "text_fields = [\"Title\", \"Content\", \"Summary\", \"Text\", \"text\", \"headline\", \"body\"]\n",
        "for doc in all_documents:\n",
        "    text = \" \".join([doc[field].strip() for field in text_fields if field in doc and isinstance(doc[field], str)]).strip()\n",
        "    if text:\n",
        "        processed_docs.append({\"text\": text})\n",
        "corpus = [doc[\"text\"] for doc in processed_docs]\n",
        "embeddings = model.encode(corpus, convert_to_numpy=True).astype(\"float32\")\n",
        "dimension = embeddings.shape[1]\n",
        "faiss_index = faiss.IndexFlatIP(dimension)\n",
        "faiss_index.add(embeddings)"
      ],
      "metadata": {
        "id": "uyrfRpHVAttt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@lru_cache(maxsize=1000)\n",
        "def embed_query(query: str):\n",
        "    return model.encode(query).astype('float32')\n",
        "\n",
        "def load_llm():\n",
        "    model_name = \"google/flan-t5-base\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "    pipe = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=200)\n",
        "    return HuggingFacePipeline(pipeline=pipe, model_kwargs={\"temperature\": 0.7})\n",
        "\n",
        "def run_rag(query, faiss_index, documents, embed_model, k=7):\n",
        "    start_time = time.time()\n",
        "    query_vec = embed_query(query).reshape(1, -1)\n",
        "    D, I = faiss_index.search(query_vec, k)\n",
        "    retrieved = [documents[i][\"text\"][:1000] for i in I[0] if i < len(documents)]\n",
        "    context = \"\\n\\n\".join(retrieved)\n",
        "    prompt = PromptTemplate(\n",
        "        input_variables=[\"context\", \"question\"],\n",
        "        template=\"\"\"\n",
        "        Use the following financial information to answer the user's question clearly and factually. Cite sources by number. If context lacks specific information, state so.\n",
        "        Context: {context}\n",
        "        Question: {question}\n",
        "        Answer (with numbered citations):\n",
        "        \"\"\"\n",
        "    )\n",
        "    llm = load_llm()\n",
        "    rag_chain = prompt | llm\n",
        "    result = rag_chain.invoke({\"context\": context, \"question\": query})\n",
        "    latency = time.time() - start_time\n",
        "    return result, retrieved, latency"
      ],
      "metadata": {
        "id": "SHQesmlKAwnY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=== Testing Diverse Financial Queries ===\")\n",
        "queries = [\n",
        "    \"What are the risks of Amazon’s $15B warehouse expansion?\",\n",
        "    \"What is the impact of Trump’s tariffs on inflation?\",\n",
        "    \"Apple is acquiring a startup in New York.\"\n",
        "]\n",
        "for query in queries:\n",
        "    response, retrieved_docs, latency = run_rag(query, faiss_index, processed_docs, model)\n",
        "    print(f\"\\nQuery: {query}\")\n",
        "    print(f\"Answer:\\n{response}\")\n",
        "    print(f\"Retrieved (Top 3):\\n{[doc[:100] + '...' for doc in retrieved_docs[:3]]}\")\n",
        "    print(f\"Latency: {latency:.2f}s\")\n",
        "\n",
        "print(\"\\n=== Fine-Tuning Retrieval ===\")\n",
        "query = \"What are the risks of Amazon’s $15B warehouse expansion?\"\n",
        "response, retrieved_docs, latency = run_rag(query, faiss_index, processed_docs, model, k=3)\n",
        "print(f\"Query: {query}\")\n",
        "print(f\"Answer (k=3):\\n{response}\")\n",
        "print(f\"Retrieved (Top 3):\\n{[doc[:100] + '...' for doc in retrieved_docs[:3]]}\")\n",
        "print(f\"Latency: {latency:.2f}s\")\n"
      ],
      "metadata": {
        "id": "R0yTmff2AzAc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7a98602-6968-47ef-d443-621f038e1308"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Testing Diverse Financial Queries ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Query: What are the risks of Amazon’s $15B warehouse expansion?\n",
            "Answer:\n",
            "I think there is still a huge downside risk for the US stock market, despite the exemption for electronics Amazon Cancels Inventory Orders From China After Tariffs ECB cuts rates again to help economy weather erratic U.S. trade policy\n",
            "Retrieved (Top 3):\n",
            "['Amazon considers $15 billion warehouse expansion plan, Bloomberg News reports...', 'I think there is still a huge downside risk for the US stock market, despite the exemption for elect...', 'Amazon Cancels Inventory Orders From China After Tariffs...']\n",
            "Latency: 6.86s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Query: What is the impact of Trump’s tariffs on inflation?\n",
            "Answer:\n",
            "controlling inflation and supporting economic growth.\n",
            "Retrieved (Top 3):\n",
            "['TRUMPONOMICS: BIG TARIFFS, BIGGER DEFICITS...', 'Powell indicates tariffs could pose a challenge for the Fed between controlling inflation and suppor...', 'What is this gonna look like after 104% tariffs？...']\n",
            "Latency: 3.05s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Query: Apple is acquiring a startup in New York.\n",
            "Answer:\n",
            "No.\n",
            "Retrieved (Top 3):\n",
            "['Apple redirects iPhone production to India amidst high China tariffs....', 'Bad Apple...', 'Apple is charting flights for 600 tons of iPhones from India...']\n",
            "Latency: 1.66s\n",
            "\n",
            "=== Fine-Tuning Retrieval ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query: What are the risks of Amazon’s $15B warehouse expansion?\n",
            "Answer (k=3):\n",
            "downside risk for the US stock market\n",
            "Retrieved (Top 3):\n",
            "['Amazon considers $15 billion warehouse expansion plan, Bloomberg News reports...', 'I think there is still a huge downside risk for the US stock market, despite the exemption for elect...', 'Amazon Cancels Inventory Orders From China After Tariffs...']\n",
            "Latency: 1.69s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n=== Optimize and Finalize ===\")\n",
        "nlist = 100\n",
        "quantizer = faiss.IndexFlatIP(dimension)\n",
        "optimized_index = faiss.IndexIVFFlat(quantizer, dimension, nlist, faiss.METRIC_INNER_PRODUCT)\n",
        "optimized_index.train(embeddings)\n",
        "optimized_index.add(embeddings)\n",
        "\n",
        "response, retrieved_docs, latency = run_rag(query, optimized_index, processed_docs, model)\n",
        "print(f\"Query (Optimized): {query}\")\n",
        "print(f\"Answer:\\n{response}\")\n",
        "print(f\"Latency: {latency:.2f}s\")"
      ],
      "metadata": {
        "id": "uXlzk3OfA3cQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "567548b4-873c-4f88-dbb8-2da0e4d358a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Optimize and Finalize ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query (Optimized): What are the risks of Amazon’s $15B warehouse expansion?\n",
            "Answer:\n",
            "ECB cuts rates again to help economy weather erratic U.S. trade policy Companies with larger fixed USD debt?\n",
            "Latency: 3.55s\n"
          ]
        }
      ]
    }
  ]
}